# ==============================================================
# üë• K-Nearest Neighbors (KNN) on Social Network Ads Dataset
# ==============================================================

# Step 1Ô∏è‚É£: Import Required Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, classification_report

# ==============================================================
# Step 2Ô∏è‚É£: Load Dataset
# ==============================================================
df = pd.read_csv("Social_Network_Ads.csv")   # Make sure file is uploaded in the same directory
print("‚úÖ Dataset Loaded Successfully!\n")
display(df.head())

# Check basic info
print("\nüìä Dataset Information:")
print(df.info())

# ==============================================================
# Step 3Ô∏è‚É£: Data Preprocessing
# ==============================================================
# Drop irrelevant columns if present (like 'User ID')
if 'User ID' in df.columns:
    df = df.drop(['User ID'], axis=1)

# Convert categorical 'Gender' into numeric form
if 'Gender' in df.columns:
    df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})

# Define independent (X) and dependent (y) variables
X = df[['Age', 'EstimatedSalary']]   # commonly used features for this dataset
y = df['Purchased']                  # target variable (0 = Not Purchased, 1 = Purchased)

# ==============================================================
# Step 4Ô∏è‚É£: Split Data into Train and Test Sets
# ==============================================================
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

print(f"Training samples: {len(X_train)}, Testing samples: {len(X_test)}")

# ==============================================================
# Step 5Ô∏è‚É£: Feature Scaling
# ==============================================================
# KNN uses distance between points ‚Äî so scale features for equal weightage
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

print("\n‚úÖ Features Scaled using StandardScaler")

# ==============================================================
# Step 6Ô∏è‚É£: Train KNN Model
# ==============================================================
# Initialize model with K = 5 (can tune later)
knn = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)  # p=2 ‚Üí Euclidean Distance
knn.fit(X_train, y_train)

print("\n‚úÖ KNN Model Trained Successfully!")

# ==============================================================
# Step 7Ô∏è‚É£: Predict Test Results
# ==============================================================
y_pred = knn.predict(X_test)

# ==============================================================
# Step 8Ô∏è‚É£: Evaluate Model
# ==============================================================
cm = confusion_matrix(y_test, y_pred)
acc = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
error_rate = 1 - acc

print("\nüéØ Model Performance Metrics:")
print(f"Accuracy      : {acc*100:.2f}%")
print(f"Error Rate    : {error_rate*100:.2f}%")
print(f"Precision     : {precision*100:.2f}%")
print(f"Recall        : {recall*100:.2f}%")

# Detailed classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# ==============================================================
# Step 9Ô∏è‚É£: Visualize Confusion Matrix
# ==============================================================
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Purples',
            xticklabels=['Not Purchased (0)','Purchased (1)'],
            yticklabels=['Not Purchased (0)','Purchased (1)'])
plt.title("Confusion Matrix - KNN Classifier")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ==============================================================
# Step üîü: Visualize Decision Boundary (Optional)
# ==============================================================
# Visualize how KNN separates the classes
from matplotlib.colors import ListedColormap

X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(
    np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),
    np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01)
)

plt.figure(figsize=(7,6))
plt.contourf(X1, X2, knn.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha=0.75, cmap=ListedColormap(('salmon', 'lightgreen')))

plt.scatter(X_set[:, 0], X_set[:, 1], c=y_set, edgecolors='k', cmap=ListedColormap(('red', 'green')))
plt.title("KNN Decision Boundary")
plt.xlabel("Age (Scaled)")
plt.ylabel("Estimated Salary (Scaled)")
plt.show()




#viva
| **Question**                       | **Answer (say clearly)**                                                                                                                                                      |
| ---------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| What is KNN?                       | KNN stands for *K-Nearest Neighbors*. It‚Äôs a **supervised classification algorithm** that classifies a new sample based on the **majority class of its nearest K neighbors**. |
| What type of learning is KNN?      | It‚Äôs a **lazy learning** algorithm ‚Äî it doesn‚Äôt build a model during training, it stores all data and classifies at prediction time.                                          |
| Is KNN supervised or unsupervised? | **Supervised**, because it requires labeled data for training.                                                                                                                |
| What does ‚ÄúK‚Äù represent in KNN?    | The number of nearest neighbors considered for voting when classifying a new point.                                                                                           |
| What happens if K = 1?             | The new point is classified based on its **single nearest neighbor** ‚Äî can lead to overfitting.                                                                               |
| What if K is too large?            | The model becomes too smooth and may **underfit**, misclassifying points.                                                                                                     |
| What is the ideal K value?         | It depends on the dataset ‚Äî typically chosen using **cross-validation** or by observing the **accuracy vs K plot**.                                                           |



workin- 
Choose K ‚Üí let‚Äôs take K = 3

Find distances of P from all points (using Euclidean distance):
| From | Distance                        | Class |
| ---- | ------------------------------- | ----- |
| A    | ‚àö((3‚àí1)¬≤ + (2‚àí1)¬≤) = ‚àö5 ‚âà 2.24  | Red   |
| B    | ‚àö((3‚àí2)¬≤ + (2‚àí2)¬≤) = 1          | Red   |
| C    | ‚àö((3‚àí3)¬≤ + (2‚àí3)¬≤) = 1          | Blue  |
| D    | ‚àö((3‚àí6)¬≤ + (2‚àí5)¬≤) = ‚àö18 ‚âà 4.24 | Blue  |
Pick the 3 nearest neighbors ‚Üí B (Red), C (Blue), A (Red)

Vote / Majority class among neighbors ‚Üí Red (2 votes), Blue (1 vote)

‚úÖ Predicted class of P = Red


| **Question**                                   | **Answer**                                                                                                         |
| ---------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ |
| What is meant by Lazy Learner?                 | KNN doesn‚Äôt learn a model; it memorizes data and performs computation during prediction.                           |
| What is Non-parametric?                        | KNN doesn‚Äôt assume any distribution about data (e.g., linear, normal).                                             |
| What‚Äôs the time complexity of KNN?             | O(n) per prediction because it compares with all training samples.                                                 |
| What‚Äôs the space complexity?                   | O(n) ‚Äî all training data must be stored in memory.                                                                 |
| How is KNN different from Logistic Regression? | Logistic regression builds a mathematical model (parametric); KNN classifies by local similarity (non-parametric). |
| Is KNN sensitive to outliers?                  | Yes ‚Äî outliers can distort neighbor distances.                                                                     |
| Why do we scale data?                          | Because distance-based algorithms can be biased toward features with larger numerical ranges.                      |


What happens if you increase K?	Boundary becomes smoother (less sensitive to noise) but may misclassify edge points.
What happens if you decrease K?	Boundary becomes more irregular ‚Äî may overfit to training data.
# ==============================================================
# üå∏ Boosting Algorithms Comparison on Iris Dataset
# ==============================================================
# Algorithms used:
# 1Ô∏è‚É£ AdaBoost (Adaptive Boosting)
# 2Ô∏è‚É£ Gradient Boosting (GBM)
# 3Ô∏è‚É£ XGBoost (Extreme Gradient Boosting)
# ==============================================================
# Step-by-step with detailed explanations and evaluation metrics.
# ==============================================================

# Step 1Ô∏è‚É£: Import Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier
from xgboost import XGBClassifier

# ==============================================================
# Step 2Ô∏è‚É£: Load and Prepare the Iris Dataset
# ==============================================================
iris = load_iris()
X = iris.data
y = iris.target

print("‚úÖ Dataset Loaded Successfully")
print("Feature Names:", iris.feature_names)
print("Target Classes:", iris.target_names)

# Split into training & testing data (80%-20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ==============================================================
# Step 3Ô∏è‚É£: Initialize and Train the Models
# ==============================================================

# 1Ô∏è‚É£ AdaBoost Classifier
# AdaBoost trains weak learners sequentially and assigns higher weights to misclassified samples
ada = AdaBoostClassifier(
    estimator=DecisionTreeClassifier(max_depth=1),  # weak learner (stump)
    n_estimators=100,                               # number of weak learners
    learning_rate=0.5,
    random_state=42
)
ada.fit(X_train, y_train)
ada_pred = ada.predict(X_test)

# 2Ô∏è‚É£ Gradient Boosting Classifier (GBM)
# GBM builds trees sequentially where each new tree corrects errors of the previous one
gbm = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)
gbm.fit(X_train, y_train)
gbm_pred = gbm.predict(X_test)

# 3Ô∏è‚É£ XGBoost Classifier
# XGBoost is an optimized version of GBM with regularization and parallel processing
xgb = XGBClassifier(
    n_estimators=100, #n_estimators defines the number of trees in the ensemble. Too few means underfitting; too many can cause overfitting. Optimal value depends on dataset and #learning rate.
    learning_rate=0.1,
    max_depth=3,
    random_state=42,
    use_label_encoder=False,
    eval_metric='mlogloss'
)
xgb.fit(X_train, y_train)
xgb_pred = xgb.predict(X_test)

# ==============================================================
# Step 4Ô∏è‚É£: Evaluate All Models
# ==============================================================

def evaluate_model(name, y_true, y_pred):
    print(f"\nüìä Model: {name}")
    print(f"Accuracy: {accuracy_score(y_true, y_pred)*100:.2f}%")
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=iris.target_names))
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(4,3))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')
    plt.title(f"Confusion Matrix - {name}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

# Evaluate each model
evaluate_model("AdaBoost", y_test, ada_pred)
evaluate_model("Gradient Boosting", y_test, gbm_pred)
evaluate_model("XGBoost", y_test, xgb_pred)

# ==============================================================
# Step 5Ô∏è‚É£: Compare All Models
# ==============================================================

accuracy_scores = {
    'AdaBoost': accuracy_score(y_test, ada_pred),
    'GradientBoosting': accuracy_score(y_test, gbm_pred),
    'XGBoost': accuracy_score(y_test, xgb_pred)
}

plt.figure(figsize=(6,4))
sns.barplot(x=list(accuracy_scores.keys()), y=list(accuracy_scores.values()), palette='Set2')
plt.title("Model Accuracy Comparison")
plt.ylabel("Accuracy")
plt.ylim(0,1)
plt.show()

print("\n‚úÖ Final Model Comparison:")
for model, acc in accuracy_scores.items():
    print(f"{model}: {acc*100:.2f}% accuracy")






#viva
| **Question**                                             | **Answer (say confidently)**                                                                                                                                                                                                     |
| -------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| What is Boosting?                                        | Boosting is an **ensemble learning technique** that combines multiple **weak learners** (like shallow trees) sequentially to form a **strong classifier**. Each new model tries to correct the errors made by the previous ones. |
| What is a Weak Learner?                                  | A weak learner is a model that performs slightly better rather than random guessing ‚Äî for example, a **Decision Tree with max depth = 1** (called a *Decision Stump*).                                                                  |
| What is the key difference between Bagging and Boosting? | Bagging builds models in **parallel** and reduces **variance**, while Boosting builds models **sequentially** and reduces **bias**.                                                                                              |
| Why do we use Boosting?                                  | To improve model accuracy by combining many simple models that correct each other‚Äôs mistakes.                                                                                                                                    |




adaboost (adaptive boosting)
| **Question**                                  | **Answer**                                                                                                                                    |
| --------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------- |
| What is AdaBoost?                             | AdaBoost is a sequential ensemble technique where each weak learner is trained by **focusing more on the previously misclassified samples**.  |
| Why is it called ‚ÄúAdaptive‚Äù?                  | Because it **adapts** the weights of data points ‚Äî increasing weights for misclassified points, and decreasing for correctly classified ones. |
| What base learner does AdaBoost use?          | Usually a **Decision Tree Stump** (depth = 1).                                                                                                |
| What are its main hyperparameters?            | `n_estimators` (number of weak learners) and `learning_rate` (controls contribution of each learner).                                         |                   
| What is the drawback of AdaBoost?             | It is **sensitive to noise and outliers** ‚Äî since they get very high weights in later iterations.                                             |



Gradient Boosting
| **Question**                          | **Answer**                                                                                                                                                                                               |
| ------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| What is Gradient Boosting?            | Gradient Boosting builds models sequentially ‚Äî each new model tries to **reduce the residual errors** (difference between actual and predicted values) of the previous model using **gradient descent**. |
| What is the key idea?                 | Instead of reweighting samples (like AdaBoost), GBM fits the next tree to the **negative gradient** (residuals) of the loss function.                                                                    |
| What is the loss function used?       | For classification: Log loss or deviance <br> For regression: Mean Squared Error (MSE).                                                                                                                  |
| Why is it called ‚ÄúGradient‚Äù Boosting? | Because it minimizes the loss function using **gradient descent optimization**.                                                                                                                          |
| What are its main hyperparameters?    | `n_estimators`, `learning_rate`, and `max_depth`.                                                                                                                                                        |
| What is the disadvantage of GBM?      | It‚Äôs slower and prone to **overfitting** if too many trees (large `n_estimators`) or deep trees are used.                                                                                                |



| **Question**                               | **Answer**                                                                                                                                                    |
| ------------------------------------------ | --------------------------------------------------------------------------------------------------------------------- | | ----------------------------------------- |
| What is XGBoost?                           | XGBoost is an optimized and **regularized version** of Gradient Boosting ‚Äî designed for speed and better performance.                                       |
| Why is it faster?                          | It builds trees in **parallel**, uses **optimized data structures**, and applies **pruning** for faster convergence.  |                                         |
| What kind of regularization does it use?   | Both **L1 (Lasso)** and **L2 (Ridge)** regularization to prevent overfitting.                                                                              
| What are its advantages?                   | 1Ô∏è‚É£ Faster than GBM  2Ô∏è‚É£ Less overfitting  3Ô∏è‚É£ Handles missing data > 4Ô∏è‚É£ Supports parallelism.                   
| What are its hyperparameters?              | `n_estimators`, `learning_rate`, `max_depth`, `lambda` (L2), `alpha` (L1).                                            




all diff
| **Feature**             | **AdaBoost**                    | **Gradient Boosting (GBM)** | **XGBoost**               |
| ----------------------- | ------------------------------- | --------------------------- | ------------------------- |
| Working Principle       | Reweights misclassified samples | Fits to residual errors     | Regularized, parallel GBM |
| Base Learner            | Shallow tree (stump)            | Decision tree               | Decision tree             |
| Regularization          | ‚ùå None                          | ‚ùå None                      | ‚úÖ L1 & L2                 |
| Speed                   | Fast                            | Medium                      | Fastest                   |
| Handles Missing Values  | ‚ùå No                            | ‚ùå No                        | ‚úÖ Yes                     |
| Overfitting Control     | Low                             | Medium                      | High                      |
| Typical Accuracy (Iris) | ~96%                            | ~97%                        | ~99%                      |

# ===============================================================
# üå∏ Linear Discriminant Analysis (LDA) on Iris Dataset
# ===============================================================
# Objective:
# To apply the Linear Discriminant Analysis algorithm on the Iris dataset
# and classify the given flower into one of the three species:
#   - Setosa
#   - Versicolor
#   - Virginica
# ===============================================================

# Step 1Ô∏è‚É£: Import all required libraries
import pandas as pd                # For data handling
import numpy as np                 # For numerical operations
import seaborn as sns              # For data visualization
import matplotlib.pyplot as plt    # For plotting graphs

from sklearn.datasets import load_iris                      # Load Iris dataset
from sklearn.model_selection import train_test_split         # Split data
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# ===============================================================
# Step 2Ô∏è‚É£: Load the Iris dataset
# ===============================================================
iris = load_iris()   # loads inbuilt dataset

# Create a DataFrame for better readability
df = pd.DataFrame(iris.data, columns=iris.feature_names)

# Add a target column (species)
df['species'] = iris.target

# Map numerical target values to species names
df['species'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})

# Display first few rows
print("‚úÖ Dataset Loaded Successfully!\n")
display(df.head())

# ===============================================================
# Step 3Ô∏è‚É£: Separate features (X) and labels (y)
# ===============================================================
X = df.iloc[:, 0:4]       # independent variables ‚Üí 4 flower measurements
y = df['species']         # dependent variable ‚Üí species name

# ===============================================================
# Step 4Ô∏è‚É£: Split the data into training and testing sets
# ===============================================================
# 80% data for training, 20% for testing
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(f"Training samples: {len(X_train)}")
print(f"Testing samples : {len(X_test)}")

# ===============================================================
# Step 5Ô∏è‚É£: Initialize and train the LDA model
# ===============================================================
lda = LinearDiscriminantAnalysis()   # create LDA model object
lda.fit(X_train, y_train)            # train the model using training data

print("\n‚úÖ LDA Model Trained Successfully!")

# ===============================================================
# Step 6Ô∏è‚É£: Predict the species for test data
# ===============================================================
y_pred = lda.predict(X_test)    # predictions on test dataset

# ===============================================================
# Step 7Ô∏è‚É£: Evaluate the model performance
# ===============================================================
accuracy = accuracy_score(y_test, y_pred)
print("\nüéØ Model Evaluation:")
print(f"Model Accuracy: {accuracy*100:.2f}%")

# Generate classification report (precision, recall, f1-score)
print("\nüìã Classification Report:")
print(classification_report(y_test, y_pred))

# ===============================================================
# Step 8Ô∏è‚É£: Visualize the Confusion Matrix
# ===============================================================
cm = confusion_matrix(y_test, y_pred)    # compute confusion matrix

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',
            xticklabels=iris.target_names,
            yticklabels=iris.target_names)

plt.title("üå∏ Confusion Matrix - LDA on Iris Dataset")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# Interpretation of Confusion Matrix:
# - Diagonal values ‚Üí Correct classifications
# - Off-diagonal values ‚Üí Misclassifications
# For example: if 'setosa' row has [10,0,0], all 10 setosa flowers classified correctly.

# ===============================================================
# Step 9Ô∏è‚É£: Visualize the LDA Projection (2D)
# ===============================================================
# LDA reduces 4D data to 2D for visualization
X_lda = lda.transform(X)

plt.figure(figsize=(8,6))
sns.scatterplot(x=X_lda[:, 0], y=X_lda[:, 1], hue=y, palette="Set1", s=80)
plt.title("üåº LDA Projection of Iris Dataset (2D View)")
plt.xlabel("Linear Discriminant 1")
plt.ylabel("Linear Discriminant 2")
plt.grid(True)
plt.show()

# Interpretation:
# Each color represents one species.
# The clearer the separation between color groups ‚Üí better classification boundary.

# ===============================================================
# Step üîü: Predict the species for a new flower
# ===============================================================
# Example new flower measurement [sepal length, sepal width, petal length, petal width]
new_flower = np.array([[5.5, 3.5, 1.3, 0.2]])

predicted_species = lda.predict(new_flower)
print(f"\nüå∏ Predicted Species for given flower: {predicted_species[0]}")

# ===============================================================
# ‚úÖ Final Observation:
# LDA achieved high accuracy (~97‚Äì100%) on the Iris dataset.
# It successfully separated the three species in a 2D space using linear combinations of features.
# ===============================================================




#viva
What is LDA?

Linear Discriminant Analysis (LDA) is a supervised machine learning algorithm used for:

Classification (assigning a sample to a class), and

Dimensionality reduction (reducing number of input features while keeping class information).

It tries to find the best line or plane that separates multiple classes in your data.

‚úÖ Example (Iris Dataset):

We have 4 features (Sepal length, Sepal width, Petal length, Petal width).

LDA tries to combine them mathematically into 2 new features (LD1 and LD2)
‚Äî so that each flower species (Setosa, Versicolor, Virginica) forms a clearly separated cluster

Step 1: Compute the Mean of Each Class

LDA first calculates the mean vector (average point) for each class.
Example: mean of all ‚Äúsetosa‚Äù samples, mean of all ‚Äúversicolor‚Äù samples, etc.

Step 2: Compute the Overall Mean

Then it finds the overall mean of the entire dataset.

Step 3: Compute Scatter Matrices

There are two important matrices:

Matrix	                         Meaning	                        Goal
Within-Class Scatter (SW)	Spread of samples within each class	Should be minimized
Between-Class Scatter (SB)	Spread of mean values between classes	Should be maximized



| Feature       | **LDA**                              | **PCA**                               |
| ------------- | ------------------------------------ | ------------------------------------- |
| Type          | **Supervised**                       | **Unsupervised**                      |
| Goal          | Maximize **class separability**      | Maximize **data variance**            |
| Uses labels?  | ‚úÖ Yes                                | ‚ùå No                                  |
| Focus         | Between-class & within-class scatter | Variance across all data              |
| Output        | Linear discriminants                 | Principal components                  |
| Best used for | Classification                       | Feature extraction / data compression |

‚ÄúLDA is preferred when we have labeled data and want to improve classification performance.
PCA is used when we only want to reduce dimensions"

1Ô∏è‚É£ How many discriminant axes can LDA create?	Minimum of (number of features, number of classes ‚àí 1). For Iris ‚Üí min(4, 3‚àí1) = 2 discriminants.
12Ô∏è‚É£ What is meant by ‚Äúprojection‚Äù in LDA?	Projecting high-dimensional data onto lower-dimensional axes (LD1, LD2) that maximize class separation.
13Ô∏è‚É£ How is LDA different from Logistic Regression?	LDA finds projection boundaries, Logistic Regression finds decision boundaries.

Projection Boundary (LDA)
The line/axis onto which all data points are projected (flattened) to maximize class separation.

Decision Boundary (Logistic Regression, SVM, etc.)	
The actual boundary that divides one class from another ‚Äî the point where the classifier ‚Äúdecides‚Äù class A vs class B.